{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZpQySyCgRrHk",
        "outputId": "08b02f81-d8ec-4c33-bb6f-68c5d27c6f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Running Notebook No.4 to build a Transformer NMT.\n",
            "================================================================================\n",
            "\n",
            "1. Mounting Google Drive...\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "2. Loading tokenizers and padded data...\n",
            "Done.\n",
            "\n",
            "3. Splitting data into training (70%), validation (10%), and test (20%) sets...\n",
            "Done.\n",
            "\n",
            "4. Building Transformer model...\n",
            "Done.\n",
            "\n",
            "5. Model training...\n",
            "Epoch 1, Training Loss: 3.9651, Validation Loss: 2.6072, Time: 0.63 min\n",
            "Epoch 2, Training Loss: 2.2088, Validation Loss: 2.1881, Time: 0.46 min\n",
            "Epoch 3, Training Loss: 1.7863, Validation Loss: 2.0648, Time: 0.46 min\n",
            "Epoch 4, Training Loss: 1.5504, Validation Loss: 2.0264, Time: 0.46 min\n",
            "Epoch 5, Training Loss: 1.3913, Validation Loss: 2.0250, Time: 0.46 min\n",
            "Epoch 6, Training Loss: 1.2699, Validation Loss: 2.0263, Time: 0.46 min\n",
            "Epoch 7, Training Loss: 1.1760, Validation Loss: 2.0492, Time: 0.46 min\n",
            "Epoch 8, Training Loss: 1.1001, Validation Loss: 2.0681, Time: 0.46 min\n",
            "Training completed. Best model from epoch 5 with Validation Loss = 2.0250\n",
            "Best model saved to: /content/gdrive/My Drive/NMT_Data/NMT_Models/Transformer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model performance on test data...\n",
            "\n",
            "Test Loss: 2.0110\n",
            "Test Perplexity: 7.4711\n",
            "\n",
            "6. Creating a translator instance with the trained Transformer...\n",
            "Done.\n",
            "\n",
            "7. Let's use the translator on a few examples...\n",
            "Example 1\n",
            "To be translated: 戸 に は 新しく ペンキ が 塗っ て あっ た 。\n",
            "Reference translation: there was a new coat of paint on the door\n",
            "Transformer output: the door was new paint\n",
            "BLEU Score: 5.26\n",
            "--------------------------------------------------------------------------------\n",
            "Example 2\n",
            "To be translated: 早く 起き なさい よ 、 じゃ ない と 遅刻 し ちゃう から ね 。\n",
            "Reference translation: get up early or you'll be late\n",
            "Transformer output: get up early you don't be late\n",
            "BLEU Score: 20.56\n",
            "--------------------------------------------------------------------------------\n",
            "Example 3\n",
            "To be translated: 僕 は 都会 の 生活 向き に 出来 て い ない と 思う 。\n",
            "Reference translation: i don't think that i'm cut out for city life\n",
            "Transformer output: i don't think i'm able to live in city\n",
            "BLEU Score: 12.07\n",
            "--------------------------------------------------------------------------------\n",
            "Example 4\n",
            "To be translated: 不安 だ 。\n",
            "Reference translation: i'm feeling nervous\n",
            "Transformer output: i'm anxious about it\n",
            "BLEU Score: 8.03\n",
            "--------------------------------------------------------------------------------\n",
            "Example 5\n",
            "To be translated: 今夜 は 外食 し たく ない 。\n",
            "Reference translation: i don't feel like eating out this evening\n",
            "Transformer output: i don't want to eat out tonight\n",
            "BLEU Score: 6.70\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "Notebook 4 completed. How does the Transformer NMT compare to the previous models?\n",
            "What differences have you noticed in the BLEU scores and translation quality?\n",
            "Also, did you know these models are far from optimized in my notebooks?.\n",
            "If you'd like, you can try making better models yourself using different parameters.\n",
            "Well~ This is the end of my NMT series. I hope you had fun working along with me! 😊\n",
            "If you have any questions or suggestions about this repo, feel free to reach out on LinkedIn.\n",
            "Here is my profile: https://www.linkedin.com/in/saberyu/\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "* This is the last notebook for my sequence-to-sequence Neural Machine Translation (NMT) project.\n",
        "* The primary goal of this particular notebook is to compare the NMT's performance between a LSTM and a Transformer architecture.\n",
        "* In this notebook, I structured the Transformer as documented in Vaswani et al, Attention Is All You Need.\n",
        "* For more information, I highly recommend checking out TensorFlow's documentation \"Neural machine translation with a Transformer and Keras\".\n",
        "* To run this notebook, modify the save_dir path below just like in the other notebooks.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import warnings\n",
        "\n",
        "# Constants and Configuration\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "MAX_TOKENS = 128\n",
        "SAVE_DIR = '/content/gdrive/My Drive/NMT_Data'\n",
        "MODEL_SAVE_DIR = os.path.join(SAVE_DIR, 'NMT_Models/Transformer')\n",
        "\n",
        "# Model Configuration\n",
        "num_layers = 1\n",
        "d_model = 128\n",
        "dff = 256\n",
        "num_heads = 2\n",
        "dropout_rate = 0.1\n",
        "epochs = 8\n",
        "warmup_steps = 1000\n",
        "\n",
        "# Mount Google Drive\n",
        "def mount_drive():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "# Load saved data\n",
        "def load_data_and_tokenizers(save_dir):\n",
        "    padded_en = np.load(os.path.join(save_dir, 'padded_english.npy'))\n",
        "    padded_ja = np.load(os.path.join(save_dir, 'padded_japanese.npy'))\n",
        "    with open(os.path.join(save_dir, 'english_tokenizer.pkl'), 'rb') as f:\n",
        "        en_tok = pickle.load(f)\n",
        "    with open(os.path.join(save_dir, 'japanese_tokenizer.pkl'), 'rb') as f:\n",
        "        ja_tok = pickle.load(f)\n",
        "\n",
        "    vocab_size_ja = len(ja_tok.word_index) + 1\n",
        "    vocab_size_en = len(en_tok.word_index) + 1\n",
        "\n",
        "    return padded_en, padded_ja, en_tok, ja_tok, vocab_size_ja, vocab_size_en\n",
        "\n",
        "# Function to create tf.data.Dataset with tokenized sequences\n",
        "def prepare_batch(ja, en):\n",
        "    ja_tensor = tf.convert_to_tensor(ja)\n",
        "    en_tensor = tf.convert_to_tensor(en)\n",
        "\n",
        "    en_inputs = en_tensor[:, :-1]\n",
        "    en_labels = en_tensor[:, 1:]\n",
        "    return (ja_tensor, en_inputs), en_labels\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "def make_batches(x_en, x_ja):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_ja, x_en))\n",
        "    return (dataset\n",
        "            .shuffle(BUFFER_SIZE)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Split and create datasets\n",
        "def prepare_datasets(padded_en, padded_ja):\n",
        "    train_en, temp_en, train_ja, temp_ja = train_test_split(padded_en, padded_ja, test_size=0.3, random_state=42)\n",
        "    val_en, test_en, val_ja, test_ja = train_test_split(temp_en, temp_ja, test_size=2/3, random_state=42)\n",
        "\n",
        "    train_batches = make_batches(train_en, train_ja)\n",
        "    val_batches = make_batches(val_en, val_ja)\n",
        "    test_batches = make_batches(test_en, test_ja)\n",
        "\n",
        "    return (train_batches, val_batches, test_batches,\n",
        "            train_ja, train_en, val_ja, val_en, test_ja, test_en)\n",
        "\n",
        "# Positional encoding\n",
        "def positional_encoding(length, depth):\n",
        "    assert depth % 2 == 0, # depth must be even for alternating sin/cos\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis] # (length, 1)\n",
        "    dims = np.arange(depth)[np.newaxis, :] # (1, depth)\n",
        "\n",
        "    angle_rates = 1 / np.power(10000, (2 * (dims//2)) / depth)\n",
        "    angle_rads = positions * angle_rates # (length, depth)\n",
        "\n",
        "    # apply sin to even indices (0, 2, 4, ...) and cos to odd (1, 3, 5, ...)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.convert_to_tensor(angle_rads, dtype=tf.float32)\n",
        "    return pos_encoding # (length, depth)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "# Define base attention (attention -> layer norm -> residual connection)\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "# Cross Attention connecting encoder and decoder\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # cache the attention scores if needed for analysis\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "# Global Self Attention in the encoder\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "# Global Self Attention but masked (in the decoder for autoregression)\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "# Just an FFN after attention (with residual connection and layer norm)\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "# Building the encoder layer (global self attention + FFN)\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "# Build the encoder with an encoder layer and embedding\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.pos_embedding(x)  # (batch_size, seq_len, d_model)\n",
        "    x = self.dropout(x) # add dropout\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Building the decoder layer\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # (batch_size, seq_len, d_model)\n",
        "    return x\n",
        "\n",
        "# Building the decoder\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.pos_embedding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "    return x\n",
        "\n",
        "# Building the Transformer NMT\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x  = inputs\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      del logits._keras_mask # will handle masked calculations manually\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits\n",
        "\n",
        "# Custom Training (Adam, with a lr rate scheduler)\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=1000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# Set up masked loss\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "# Forward and backward pass\n",
        "@tf.function\n",
        "def train_step(model, optimizer, inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = masked_loss(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def val_step(model, inputs, labels):\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = masked_loss(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "# Custom training loop\n",
        "def custom_training_loop(model, optimizer, train_batches, val_batches, epochs, model_save_dir=None):\n",
        "    train_loss_results = []\n",
        "    val_loss_results = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_weights = None\n",
        "    best_epoch = -1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        val_loss_avg = tf.keras.metrics.Mean()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # training\n",
        "        for inputs, labels in train_batches:\n",
        "            loss = train_step(model, optimizer, inputs, labels)\n",
        "            epoch_loss_avg(loss)\n",
        "\n",
        "        # validation\n",
        "        for inputs, labels in val_batches:\n",
        "            val_loss = val_step(model, inputs, labels)\n",
        "            val_loss_avg(val_loss)\n",
        "\n",
        "        train_loss = epoch_loss_avg.result()\n",
        "        val_loss = val_loss_avg.result()\n",
        "\n",
        "        train_loss_results.append(train_loss)\n",
        "        val_loss_results.append(val_loss)\n",
        "\n",
        "        # save the temporary best model weights\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_weights = model.get_weights()\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "        # epoch summary\n",
        "        epoch_time = (time.time() - start_time) / 60\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}, \"\n",
        "              f\"Validation Loss: {val_loss:.4f}, Time: {epoch_time:.2f} min\")\n",
        "\n",
        "    # load best model\n",
        "    model.set_weights(best_weights)\n",
        "    print(f\"Training completed. Best model from epoch {best_epoch} with Validation Loss = {best_val_loss:.4f}\")\n",
        "\n",
        "    # save the final best model after training is completed\n",
        "    if model_save_dir:\n",
        "        os.makedirs(model_save_dir, exist_ok=True)\n",
        "        model_path = os.path.join(model_save_dir, 'best_transformer_model.weights.h5')\n",
        "        model.save_weights(model_path)\n",
        "        print(f\"Best model saved to: {model_save_dir}\")\n",
        "\n",
        "    return train_loss_results, val_loss_results\n",
        "\n",
        "# Plot training progress with perplexity\n",
        "def plot_perplexity(train_loss, val_loss):\n",
        "    train_perplexity = np.exp(train_loss)\n",
        "    val_perplexity = np.exp(val_loss)\n",
        "\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.plot(range(1, len(train_perplexity)+1), train_perplexity, label='train', linestyle='dotted', color='blue')\n",
        "    plt.plot(range(1, len(val_perplexity)+1), val_perplexity, label='val', linestyle='solid', color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.legend()\n",
        "    plt.title(\"Perplexity Vs. Epoch\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluation on test dataset\n",
        "def evaluate_on_test(model, test_batches):\n",
        "    test_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "    for inputs, labels in test_batches:\n",
        "        test_loss = val_step(model, inputs, labels)\n",
        "        test_loss_avg(test_loss)\n",
        "\n",
        "    print(f\"\\nTest Loss: {test_loss_avg.result():.4f}\")\n",
        "    print(f\"Test Perplexity: {np.exp(test_loss_avg.result()):.4f}\")\n",
        "\n",
        "# Build a translator class\n",
        "class Translator(tf.Module):\n",
        "    def __init__(self, ja_tok, en_tok, transformer, start_token='start', end_token='end', max_tokens=100):\n",
        "        self.ja_tok = ja_tok\n",
        "        self.en_tok = en_tok\n",
        "        self.transformer = transformer\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.start_token_id = en_tok.word_index[start_token]\n",
        "        self.end_token_id = en_tok.word_index[end_token]\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def __call__(self, sentence):\n",
        "        if isinstance(sentence, str):\n",
        "            sentence = [sentence]\n",
        "\n",
        "        # convert Japanese sentence to input tensor\n",
        "        encoder_input = self.ja_tok.texts_to_sequences(sentence)\n",
        "        encoder_input = tf.keras.preprocessing.sequence.pad_sequences(encoder_input, padding='post')\n",
        "        encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "\n",
        "        # initialize output with the [START] token\n",
        "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, tf.constant([self.start_token_id], dtype=tf.int64))\n",
        "\n",
        "        for i in tf.range(self.max_tokens):\n",
        "            output = tf.transpose(output_array.stack())  # (1, current_seq_len)\n",
        "            predictions = self.transformer([encoder_input, output], training=False)\n",
        "            predictions = predictions[:, -1:, :]  # get last time step\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            output_array = output_array.write(i + 1, predicted_id[0])\n",
        "\n",
        "            if predicted_id[0][0].numpy() == self.end_token_id:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())  # (1, seq_len)\n",
        "        output_tokens = output.numpy()[0]\n",
        "\n",
        "        # convert token ids back to words\n",
        "        index_word = self.en_tok.index_word\n",
        "        translated_tokens = [index_word.get(id, '') for id in output_tokens if id != 0]\n",
        "        translated_text = ' '.join(translated_tokens[1:-1])  # skip <start> and <end> for readability\n",
        "\n",
        "        return translated_text, translated_tokens\n",
        "\n",
        "# Calculate BLEU score for translated text\n",
        "def rate_bleu_score(reference_text, translated_text, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "    smoother = SmoothingFunction().method1\n",
        "    reference_tokens = reference_text.split()\n",
        "    candidate_tokens = translated_text.split()\n",
        "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, weights=weights, smoothing_function=smoother)\n",
        "    return bleu_score\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Running Notebook No.4 to build a Transformer NMT.\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Suppress User Warnings\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "    # Mount Google Drive\n",
        "    print(\"\\n1. Mounting Google Drive...\")\n",
        "    mount_drive()\n",
        "\n",
        "    # Load data from Drive\n",
        "    print(\"\\n2. Loading tokenizers and padded data...\")\n",
        "    padded_en, padded_ja, en_tok, ja_tok, vocab_size_ja, vocab_size_en = load_data_and_tokenizers(SAVE_DIR)\n",
        "    print(\"Done.\")\n",
        "\n",
        "    # Prepare datasets\n",
        "    print(\"\\n3. Splitting data into training (70%), validation (10%), and test (20%) sets...\")\n",
        "    (train_batches, val_batches, test_batches,\n",
        "     train_ja, train_en, val_ja, val_en, test_ja, test_en) = prepare_datasets(padded_en, padded_ja)\n",
        "    print(\"Done.\")\n",
        "\n",
        "    # Build Transformer model\n",
        "    print(\"\\n4. Building Transformer model...\")\n",
        "    model = Transformer(\n",
        "        num_layers=num_layers,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        input_vocab_size=vocab_size_ja,\n",
        "        target_vocab_size=vocab_size_en,\n",
        "        dropout_rate=dropout_rate)\n",
        "\n",
        "    # Setup optimizer with learning rate schedule\n",
        "    learning_rate = CustomSchedule(d_model, warmup_steps=warmup_steps)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "    # Initialize model with a forward pass\n",
        "    sample_inputs, sample_labels = next(iter(train_batches))\n",
        "    _ = model(sample_inputs)\n",
        "    print(\"Done.\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n5. Model training...\")\n",
        "    train_loss, val_loss = custom_training_loop(model, optimizer, train_batches, val_batches, epochs=epochs, model_save_dir=MODEL_SAVE_DIR)\n",
        "    plot_perplexity(train_loss, val_loss)\n",
        "    print(\"Evaluating model performance on test data...\")\n",
        "    evaluate_on_test(model, test_batches)\n",
        "\n",
        "    # Create translator instance\n",
        "    print(\"\\n6. Creating a translator instance with the trained Transformer...\")\n",
        "    translator = Translator(ja_tok, en_tok, model, start_token='start', end_token='end', max_tokens=padded_en.shape[1])\n",
        "    print(\"Done.\")\n",
        "\n",
        "    # Test translation on some examples\n",
        "    print(\"\\n7. Let's use the translator on a few examples...\")\n",
        "    indices_chosen = [10, 20, 30, 40, 50]\n",
        "    for i, indice in enumerate(indices_chosen, 1):\n",
        "        japanese_test = ja_tok.sequences_to_texts([test_ja[indice]])[0].replace('start', '').replace('end', '').strip()\n",
        "        reference_translation = en_tok.sequences_to_texts([test_en[indice]])[0].replace('start', '').replace('end', '').strip()\n",
        "        print(f'Example {i}')\n",
        "        print(f'To be translated: {japanese_test}')\n",
        "        print(f'Reference translation: {reference_translation}')\n",
        "\n",
        "        # call translator\n",
        "        translated_text, translated_tokens = translator(japanese_test)\n",
        "        print(f'Transformer output: {translated_text}')\n",
        "\n",
        "        # calculate BLEU score\n",
        "        bleu_score = rate_bleu_score(reference_translation, translated_text)\n",
        "        print(f'BLEU Score: {bleu_score * 100:.2f}') #  note BLEU is usually reported in range 0-100, but it is not a percentage.\n",
        "        print('-' * 80)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Notebook 4 completed. How does the Transformer NMT compare to the previous models?\")\n",
        "    print(\"What differences have you noticed in the BLEU scores and translation quality?\")\n",
        "    print(\"Also, did you know these models are far from optimized in my notebooks?.\")\n",
        "    print(\"If you'd like, you can try making better models yourself using different parameters.\")\n",
        "    print(\"Well~ This is the end of my NMT series. I hope you had fun working along with me! 😊\")\n",
        "    print(\"If you have any questions or suggestions about this repo, feel free to reach out on LinkedIn.\")\n",
        "    print(\"Here is my profile: https://www.linkedin.com/in/saberyu/\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGhrfyYmTU8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}