{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLyf5r8aUFZ5",
        "outputId": "5924901f-d597-453f-c55b-8eb45d1b6c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Running Notebook No.1 for Data Prep.\n",
            "================================================================================\n",
            "\n",
            "1. Mounting Google Drive...\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "2. Reading data from /content/gdrive/My Drive/NMT_Data/jpn.txt...\n",
            "Done. Loaded 178348 rows of data.\n",
            "An example of the data: ÔªøLet's try something.\t‰Ωï„Åã„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ\n",
            "\n",
            "3. Preocessing the data...\n",
            "Done.\n",
            "\n",
            "4. Tagging, tokenizing, and padding the English data...\n",
            "\n",
            "  Original Sentence: let's try something .\n",
            "  Tagged Sentence: <start> let's try something . <end>\n",
            "  Padded Sentence: [  1 157 256 171   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "  Max Length: 57\n",
            "  Vocab Size: 15650\n",
            "  \n",
            "\n",
            "5. Installing MeCab for the Japanese data...\n",
            "Done.\n",
            "\n",
            "6. Tagging, tokenizing, and padding the Japanese data...\n",
            "\n",
            "  Original Sentence: ‰Ωï„Åã„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ „ÄÇ\n",
            "  Tagged Sentence: <start> ‰Ωï „Åã „Åó „Å¶ „Åø „Åæ„Åó„Çá„ÅÜ „ÄÇ <end>\n",
            "  Padded Sentence: [  1  35  18  12   9 204 186   3   2   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0]\n",
            "  Max Length: 91\n",
            "  Vocab Size: 25013\n",
            "  \n",
            "\n",
            "7. Saving Data and Tokenizers to Google Dirve...\n",
            "Done.\n",
            "\n",
            "================================================================================\n",
            "You've hit the end of this notebook. Go to the next one and start building LSTM with me. üòä\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "* This is the first notebook in a series for my sequence-to-sequence Neural Machine Translation (NMT) project.\n",
        "* The primary goal of this particular notebook is to explore the data and perform preprocessing.\n",
        "* The dataset I used was obtained from ManyThings.org (Tatoeba Project). It can be downloaded on https://www.manythings.org/corpus/.\n",
        "* To run this notebook, download the data provided with my post, upload it to your Google Drive, and modify the path in the Main function.\n",
        "* The next notebook in this series will be \"2.NMT with simple LSTM\".\n",
        "\"\"\"\n",
        "\n",
        "# Import all necessary packages for this notebook\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Mount Google Drive\n",
        "def mount_drive():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "# Load raw parallel data\n",
        "def load_raw_data(filepath):\n",
        "  with open(filepath, 'r', encoding='utf8') as f:\n",
        "      data = f.readlines()\n",
        "  return data\n",
        "\n",
        "# English preprocessing\n",
        "def preprocess_english(sentence):\n",
        "  sentence = sentence.lower().strip() # just keep everything in lower case\n",
        "  sentence = unicodedata.normalize('NFC', sentence) # standardize characters\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) # add spaces around punctuations\n",
        "  sentence = re.sub(r\"[^a-z0-9?.!,'-]+\", \" \", sentence) # keep only valid elements\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence) # remove extra spaces\n",
        "  return sentence.strip()\n",
        "\n",
        "# Japanese preprocessing\n",
        "def preprocess_japanese(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  sentence = unicodedata.normalize(\"NFKC\", sentence)\n",
        "  sentence = re.sub(r\"([?.!,?„ÄÇ!„ÄÅ])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  return sentence.strip()\n",
        "\n",
        "# Tokenize and pad English data\n",
        "def tokenize_and_pad_english(sequences):\n",
        "  \"\"\"\n",
        "  Process the English sequences by adding <start> and <end> tags and tokenizing them with paddings.\n",
        "  Returns:\n",
        "    tagged (list of str): sentences with <start> and <end> tags added.\n",
        "    padded (np.ndarray): tokenized and padded sequences.\n",
        "    tokenzier (Tokenizer): tensorflow tokenizer fitted to the data.\n",
        "    max_len (int): maximum sequence length after tokenization.\n",
        "  \"\"\"\n",
        "  tagged = [f\"<start> {s} <end>\" for s in sequences] # add indicator tags to the sentences\n",
        "  tokenizer = Tokenizer(split=' ', char_level=False) # get tokenizer\n",
        "  tokenizer.fit_on_texts(tagged) # fit tokenzier on the corpus\n",
        "  tokenized = tokenizer.texts_to_sequences(tagged) # tokenize the data\n",
        "  max_len = max(len(seq) for seq in tokenized) # get maximum sequence length out of all data\n",
        "  padded = pad_sequences(tokenized, maxlen=max_len, padding='post') # add paddings to ensure all sequences have the same length\n",
        "  return tagged, padded, tokenizer, max_len\n",
        "\n",
        "# Tokenize and pad Japanese data with MeCab\n",
        "def tokenize_and_pad_japanese(sequences):\n",
        "  \"\"\"\n",
        "  Process the Japanese sequences by breaking them down into the smallest meaningful Japanese units. Add tags and tokenize the data with paddings.\n",
        "  Returns:\n",
        "    tagged (list of str): sentences with <start> and <end> tags added.\n",
        "    padded (np.ndarray): tokenized and padded sequences.\n",
        "    tokenzier (Tokenizer): tensorflow tokenizer fitted to the data.\n",
        "    max_len (int): maximum sequence length after tokenization.\n",
        "  \"\"\"\n",
        "  import MeCab # use MeCab to break down the Japanese characters into the smallest meaningful units (a.k.a. Morphological Units in Linguistics)\n",
        "  mecab = MeCab.Tagger(\"-Owakati\") # get MeCab to return wakati (morphological japanese separated by space)\n",
        "  wakati = [mecab.parse(s).strip() for s in sequences] # apply MeCab\n",
        "  tagged = [f\"<start> {s} <end>\" for s in wakati]\n",
        "  tokenizer = Tokenizer(split=' ', char_level=False)\n",
        "  tokenizer.fit_on_texts(tagged)\n",
        "  tokenized = tokenizer.texts_to_sequences(tagged)\n",
        "  max_len = max(len(seq) for seq in tokenized)\n",
        "  padded = pad_sequences(tokenized, maxlen=max_len, padding='post')\n",
        "  return tagged, padded, tokenizer, max_len\n",
        "\n",
        "# Save data and tokenizers\n",
        "def save_preprocessed_data(padded_en, padded_ja, en_tok, ja_tok, save_dir):\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "  np.save(os.path.join(save_dir, 'padded_english.npy'), padded_en) # save the padded sequences\n",
        "  np.save(os.path.join(save_dir, 'padded_japanese.npy'), padded_ja)\n",
        "  # save the tokenizers using Pickle\n",
        "  with open(os.path.join(save_dir, 'english_tokenizer.pkl'), 'wb') as f:\n",
        "      pickle.dump(en_tok, f)\n",
        "  with open(os.path.join(save_dir, 'japanese_tokenizer.pkl'), 'wb') as f:\n",
        "      pickle.dump(ja_tok, f)\n",
        "\n",
        "def main():\n",
        "  # Intro\n",
        "  print(\"=\"*80)\n",
        "  print(\"Running Notebook No.1 for Data Prep.\")\n",
        "  print(\"=\"*80)\n",
        "\n",
        "  # Mount Google Drive\n",
        "  print(\"\\n1. Mounting Google Drive...\")\n",
        "  mount_drive()\n",
        "\n",
        "  # Access the data and print an example\n",
        "  path = '/content/gdrive/My Drive/NMT_Data/jpn.txt'\n",
        "  print(f\"\\n2. Reading data from {path}...\")\n",
        "  data = load_raw_data(path)\n",
        "  print(f\"Done. Loaded {len(data)} rows of data.\")\n",
        "  print(f'An example of the data: {data[0]}')\n",
        "\n",
        "  # Split and preprocess\n",
        "  print(\"3. Preocessing the data...\")\n",
        "  english_raw = []\n",
        "  japanese_raw = []\n",
        "  for line in data:\n",
        "      en, ja = line.split('\\t') # the sequences are split by tabs\n",
        "      english_raw.append(preprocess_english(en))\n",
        "      japanese_raw.append(preprocess_japanese(ja))\n",
        "  print(\"Done.\")\n",
        "\n",
        "  # Add <start> and <end> tags, tokenize, and pad English\n",
        "  print(\"\\n4. Tagging, tokenizing, and padding the English data...\")\n",
        "  tagged_en, padded_en, en_tok, max_len_en = tokenize_and_pad_english(english_raw)\n",
        "  print(f\"\"\"\n",
        "  Original Sentence: {english_raw[0]}\n",
        "  Tagged Sentence: {tagged_en[0]}\n",
        "  Padded Sentence: {padded_en[0]}\n",
        "  Max Length: {max_len_en}\n",
        "  Vocab Size: {len(en_tok.word_index)}\n",
        "  \"\"\")\n",
        "\n",
        "  # Install MeCab\n",
        "  print(\"\\n5. Installing MeCab for the Japanese data...\")\n",
        "  os.system('apt install -y mecab mecab-ipadic-utf8')\n",
        "  os.system('pip install mecab-python3 unidic-lite')\n",
        "  print(\"Done.\")\n",
        "\n",
        "  # Add <start> and <end> tags, tokenize, and pad Japanese\n",
        "  print(\"\\n6. Tagging, tokenizing, and padding the Japanese data...\")\n",
        "  tagged_ja, padded_ja, ja_tok, max_len_ja = tokenize_and_pad_japanese(japanese_raw)\n",
        "  print(f\"\"\"\n",
        "  Original Sentence: {japanese_raw[0]}\n",
        "  Tagged Sentence: {tagged_ja[0]}\n",
        "  Padded Sentence: {padded_ja[0]}\n",
        "  Max Length: {max_len_ja}\n",
        "  Vocab Size: {len(ja_tok.word_index)}\n",
        "  \"\"\")\n",
        "\n",
        "  # Save to files\n",
        "  print(\"\\n7. Saving Data and Tokenizers to Google Dirve...\")\n",
        "  save_dir = os.path.dirname(path)\n",
        "  save_preprocessed_data(padded_en, padded_ja, en_tok, ja_tok, save_dir)\n",
        "  print(\"Done.\")\n",
        "\n",
        "  # Indicate the end\n",
        "  print(\"\\n\" + \"=\" * 80)\n",
        "  print(\"You've hit the end of this notebook. Go to the next one and start building LSTM with me. üòä\")\n",
        "  print(\"=\"*80)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWIipaAGswRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}